[{"authors":["alexei-efros"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1722429317,"objectID":"5b89485041061941aa958e98c8e446f3","permalink":"https://excv-workshop.github.io/authors/alexei-efros/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexei-efros/","section":"authors","summary":"","tags":null,"title":"Alexei A. Efros","type":"authors"},{"authors":["robin-hesse"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"cedc073fcc62e1e35d98be6f6722751a","permalink":"https://excv-workshop.github.io/authors/robin-hesse/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/robin-hesse/","section":"authors","summary":"","tags":null,"title":"Robin Hesse","type":"authors"},{"authors":["olga-russakovsky"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1722429317,"objectID":"feb54247c7064d3ac03baf4bb4379b61","permalink":"https://excv-workshop.github.io/authors/olga-russakovsky/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/olga-russakovsky/","section":"authors","summary":"","tags":null,"title":"Olga Russakovsky","type":"authors"},{"authors":["sukrut-rao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"63249b059b9d6fce02fa939a228e3f85","permalink":"https://excv-workshop.github.io/authors/sukrut-rao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sukrut-rao/","section":"authors","summary":"","tags":null,"title":"Sukrut Rao","type":"authors"},{"authors":["moritz-boehle"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"23b595fff485b7b2ca87e9dd165c10bd","permalink":"https://excv-workshop.github.io/authors/moritz-boehle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/moritz-boehle/","section":"authors","summary":"","tags":null,"title":"Moritz Böhle","type":"authors"},{"authors":["rene-vidal"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1726517379,"objectID":"093b7132f35f3e9f5f5afc89979774df","permalink":"https://excv-workshop.github.io/authors/rene-vidal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rene-vidal/","section":"authors","summary":"","tags":null,"title":"René Vidal","type":"authors"},{"authors":["quentin-bouniot"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"ed5227bab3b13590ba42c9e1dc7e99ff","permalink":"https://excv-workshop.github.io/authors/quentin-bouniot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/quentin-bouniot/","section":"authors","summary":"","tags":null,"title":"Quentin Bouniot","type":"authors"},{"authors":["yossi-gandelsman"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1726517379,"objectID":"a1281d4138774ca94e6fb2ffb4f0ddf7","permalink":"https://excv-workshop.github.io/authors/yossi-gandelsman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yossi-gandelsman/","section":"authors","summary":"","tags":null,"title":"Yossi Gandelsman","type":"authors"},{"authors":["simone-schaub-meyer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"1e569d7a7c74ad8ff13c99ecc00cea24","permalink":"https://excv-workshop.github.io/authors/simone-schaub-meyer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/simone-schaub-meyer/","section":"authors","summary":"","tags":null,"title":"Simone Schaub-Meyer","type":"authors"},{"authors":["zeynep-akata"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1726517379,"objectID":"b7e07895fc620fbe2f347b71978b4e79","permalink":"https://excv-workshop.github.io/authors/zeynep-akata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zeynep-akata/","section":"authors","summary":"","tags":null,"title":"Zeynep Akata","type":"authors"},{"authors":["stefan-roth"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"e09a9ba53cd473b2200cd86792ba84d2","permalink":"https://excv-workshop.github.io/authors/stefan-roth/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stefan-roth/","section":"authors","summary":"","tags":null,"title":"Stefan Roth","type":"authors"},{"authors":["kate-saenko"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"fe955d724d6f0328972311a3de9fb64f","permalink":"https://excv-workshop.github.io/authors/kate-saenko/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kate-saenko/","section":"authors","summary":"","tags":null,"title":"Kate Saenko","type":"authors"},{"authors":["bernt-schiele"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1720474832,"objectID":"8a282fb9b35ba2c76e81e0de35457c32","permalink":"https://excv-workshop.github.io/authors/bernt-schiele/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bernt-schiele/","section":"authors","summary":"","tags":null,"title":"Bernt Schiele","type":"authors"},{"authors":["Sunnie S. Y. Kim","Elizabeth Anne Watkins","Olga Russakovsky","Ruth Fong","Andrés Monroy-Hernández"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725998523,"objectID":"f16077412f2026682aaf32beb19abc9e","permalink":"https://excv-workshop.github.io/publication/help-me-help-the-ai-understanding-how-explainability-can-support-human-ai-interaction/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/help-me-help-the-ai-understanding-how-explainability-can-support-human-ai-interaction/","section":"publication","summary":"Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users' explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.","tags":["nonproceedings"],"title":"\"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction","type":"publication"},{"authors":["Jiageng Zhu","Hanchen Xie","Mahyar Khayatkhoei","Jiazhi Li","Wael AbdAlmageed"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"fd8035b92deddb7896f467b6be0ed66a","permalink":"https://excv-workshop.github.io/publication/an-investigation-on-the-position-encoding-in-vision-based-dynamics-prediction/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/an-investigation-on-the-position-encoding-in-vision-based-dynamics-prediction/","section":"publication","summary":"Despite the success of vision-based dynamics prediction models, which predict object states by utilizing RGB images and simple object descriptions, they were challenged by environment misalignments. Although the literature has demonstrated that unifying visual domains with both environment context and object abstract, such as semantic segmentation and bounding boxes, can effectively mitigate the visual domain misalignment challenge, discussions were focused on the abstract of environment context, where the insight of using bounding box as the object abstract is under-explored. Furthermore, we notice that, as empirical results shown in the literature, even when the visual appearance of objects is removed, object bounding boxes alone, instead of being directly fed into the network, can indirectly provide sufficient position information via the Region of Interest Pooling operation for dynamics prediction. However, previous literature overlooked discussions regarding how such position information is implicitly encoded in the dynamics prediction model. Thus, in this paper, we provide detailed studies to investigate the process and necessary conditions for encoding position information via using the bounding box as the object abstract into output features. Furthermore, we study the limitation of solely using object abstracts, such that the dynamics prediction performance will be jeopardized when the environment context varies.","tags":["proceedings"],"title":"An Investigation on The Position Encoding in Vision-Based Dynamics Prediction","type":"publication"},{"authors":["Martina G. Vilas","Timothy Schaumlöffel","Gemma Roig"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"aaeff05949c1272906280804d9090a59","permalink":"https://excv-workshop.github.io/publication/analyzing-vision-transformers-for-image-classification-in-class-embedding-space/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/analyzing-vision-transformers-for-image-classification-in-class-embedding-space/","section":"publication","summary":"Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.","tags":["nonproceedings"],"title":"Analyzing Vision Transformers for Image Classification in Class Embedding Space","type":"publication"},{"authors":["Julia Grabinski","Janis Keuper","Margret Keuper"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"0953eeb9057a1bf0da075bf258c0bd0b","permalink":"https://excv-workshop.github.io/publication/as-large-as-it-gets-learning-infinitely-large-filters-via-neural-implicit-functions-in-the-fourier-domain/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/as-large-as-it-gets-learning-infinitely-large-filters-via-neural-implicit-functions-in-the-fourier-domain/","section":"publication","summary":"Recent work in neural networks for image classification has seen a strong tendency towards increasing the spatial context during encoding. Whether achieved through large convolution kernels or self-attention, models scale poorly with the increased spatial context, such that the improved model accuracy often comes at significant costs. In this paper, we propose a module for studying the effective filter size of convolutional neural networks (CNNs). To facilitate such a study, several challenges need to be addressed: (i) we need an effective means to train models with large filters (potentially as large as the input data) without increasing the number of learnable parameters, (ii) the employed convolution operation should be a plug-and-play module that can replace conventional convolutions in a CNN and allow for an efficient implementation in current frameworks, (iii) the study of filter sizes has to be decoupled from other aspects such as the network width or the number of learnable parameters, and (iv) the cost of the convolution operation itself has to remain manageable i.e.~we can not naïvely increase the size of the convolution kernel. To address these challenges, we propose to learn the frequency representations of filter weights as neural implicit functions, such that the better scalability of the convolution in the frequency domain can be leveraged. Additionally, due to the implementation of the proposed neural implicit function, even large and expressive spatial filters can be parameterized by only a few learnable weights. Interestingly, our analysis shows that, although the proposed networks could learn very large convolution kernels, the learned filters are well localized and relatively small in practice when transformed from the frequency to the spatial domain. We anticipate that our analysis of individually optimized filter sizes will allow for more efficient, yet effective, models in the future. Our code is available at https://github.com/GeJulia/NIFF .","tags":["nonproceedings"],"title":"As large as it gets: Learning infinitely large Filters via Neural Implicit Functions in the Fourier Domain","type":"publication"},{"authors":["Dongkyun Kim","Mingi Kwon","Youngjung Uh"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"3f38b63a2553481be9d474667168ebf8","permalink":"https://excv-workshop.github.io/publication/attribute-based-interpretable-evaluation-metrics-for-generative-models/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/attribute-based-interpretable-evaluation-metrics-for-generative-models/","section":"publication","summary":"When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond \"diversity\". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we reveal the following about existing generative models. ProjectedGAN generates implausible attribute relationships such as a baby with a beard even though it has competitive scores of existing metrics. Diffusion models struggle to capture diverse colors in the datasets. The larger sampling timesteps of latent diffusion model generate the more minor objects including earrings and necklaces. Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models.","tags":["nonproceedings"],"title":"Attribute Based Interpretable Evaluation Metrics for Generative Models","type":"publication"},{"authors":["Paul Gavrikov","Janis Keuper"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"c5e16b4498d1eb5530c7c8e8e8daf16b","permalink":"https://excv-workshop.github.io/publication/can-biases-in-imagenet-models-explain-generalization/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/can-biases-in-imagenet-models-explain-generalization/","section":"publication","summary":"The robust generalization of models to rare in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification this manifests in the existence of adversarial attacks the performance drops on distorted images and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50 we perform a large-scale study on 48 ImageNet models obtained via different training methods to understand how and if these biases - including shape bias spectral biases and critical bands - interact with generalization. Our extensive study results reveal that contrary to previous findings these biases are insufficient to accurately predict the generalization of a model holistically. We provide access to all checkpoints and evaluation code at https://github.com/paulgavrikov/biases_vs_generalization/","tags":["nonproceedings"],"title":"Can Biases in ImageNet Models Explain Generalization?","type":"publication"},{"authors":["Jae Hee Lee","Georgii Mikriukov","Gesina Schwalbe","Stefan Wermter","Diedrich Wolter"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"d2808957c80d7be170a98ce830306a3e","permalink":"https://excv-workshop.github.io/publication/concept-based-explanations-in-computer-vision-where-are-we-and-where-could-we-go/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/concept-based-explanations-in-computer-vision-where-are-we-and-where-could-we-go/","section":"publication","summary":"Concept-based XAI (C-XAI) approaches to explaining neural vision models are a promising field of research, since explanations that refer to concepts (i.e., semantically meaningful parts in an image) are intuitive to understand and go beyond saliency-based techniques that only reveal relevant regions. Given the remarkable progress in this field in recent years, it is time for the community to take a critical look at the advances and trends. Consequently, this paper reviews C-XAI methods to identify interesting and underexplored open ends and proposes future research directions. To this end, we consider three main directions: the choice of concepts to explain, the choice of concept representation, and how we can control concepts. For the latter, we propose techniques and draw inspiration from the field of knowledge representation and learning, showing how this could enrich future C-XAI research.","tags":["proceedings"],"title":"Concept-Based Explanations in Computer Vision: Where Are We and Where Could We Go?","type":"publication"},{"authors":["Hyemin Bang","Angie Boggust","Arvind Satyanarayan"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"0691402f8516a30cc8e998e0db1b0ada","permalink":"https://excv-workshop.github.io/publication/explanation-alignment-quantifying-the-correctness-of-model-reasoning-at-scale/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/explanation-alignment-quantifying-the-correctness-of-model-reasoning-at-scale/","section":"publication","summary":"To improve the reliability of machine learning models, researchers have developed metrics to measure the alignment between model saliency and human explanations. Thus far, however, these saliency-based alignment metrics have been used to conduct descriptive analyses and instance-level evaluations of models and saliency methods. To enable evaluative and comparative assessments of model alignment, we extend these metrics to compute *explanation alignment*—the aggregate agreement between model and human explanations. To compute explanation alignment, we aggregate saliency-based alignment metrics over many model decisions and report the result as a performance metric that quantifies how often model decisions are made for the right reasons. Through experiments on nearly 200 image classification models, multiple saliency methods, and MNIST, CelebA, and ImageNet tasks, we find that explanation alignment automatically identifies spurious correlations, such as model bias, and uncovers behavioral differences between nearly identical models. Further, we characterize the relationship between explanation alignment and model performance, evaluating the factors that impact explanation alignment and how to interpret its results in-practice.","tags":["proceedings"],"title":"Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale","type":"publication"},{"authors":["Hui Yu Lau","Srinandan Dasmahapatra","Hansung Kim"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"6d101539ffc3aaa9d4985306f98b8fea","permalink":"https://excv-workshop.github.io/publication/feature-contribution-in-monocular-depth-estimation/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/feature-contribution-in-monocular-depth-estimation/","section":"publication","summary":"Monocular Depth Estimation (MDE) is an inherently ill-posed problem due to the lack of binocular depth cues, despite this there has been significant progress made in this field in recent years. In an attempt to bridge understanding between human and machine perception, this paper investigates learned concepts from the general-purpose model Depth Anything, focusing on features that are known to be present in the human visual system. We perform interventions on different image features within the KITTI and NYUv2 dataset, evaluating performance on these intervened inputs. This led to interesting insights on the way and amount each of these features influences depth perception. These insights contributes to bridging understanding of how humans and machines perform MDE respectively, but we also hope it provides new ways for future work to devise more robust methods of training neural networks on MDE.","tags":["proceedings"],"title":"Feature Contribution in Monocular Depth Estimation","type":"publication"},{"authors":["Kristoffer Wickstrøm","Marina MC Höhne","Anna Hedström"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"b13f309c05e3cc9b1cc4891fdfa3fd40","permalink":"https://excv-workshop.github.io/publication/from-flexibility-to-manipulation-the-slippery-slope-of-xai-evaluation/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/from-flexibility-to-manipulation-the-slippery-slope-of-xai-evaluation/","section":"publication","summary":"The lack of ground truth explanation labels is a fundamental challenge for quantitative evaluation in explainable artificial intelligence (XAI). This challenge becomes especially problematic when evaluation methods have numerous hyperparameters that must be specified by the user, as there is no ground truth to determine an optimal hyperparameter selection. It is typically not feasible to do an exhaustive search of hyperparameters so researchers typically make a normative choice based on similar studies in the literature, which provides great flexibility for the user. In this work, we illustrate how this flexibility can be exploited to manipulate the evaluation outcome. We frame this manipulation as an adversarial attack on the evaluation where seemingly innocent changes in hyperparameter setting significantly influence the evaluation outcome. We demonstrate the effectiveness of our manipulation across several datasets with large changes in evaluation outcomes across several explanation methods and models. Lastly, we propose a mitigation strategy based on ranking across hyperparameters that aims to provide robustness towards such manipulation. This work highlights the difficulty of conducting reliable XAI evaluation and emphasizes the importance of a holistic and transparent approach to evaluation in XAI. Code is available at https://github.com/Wickstrom/quantitative-xai-manipulation.","tags":["proceedings"],"title":"From Flexibility to Manipulation: The Slippery Slope of XAI Evaluation","type":"publication"},{"authors":["Bartlomiej Sobieski","Przemyslaw Biecek"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"65d3b98fdeaa662627c537d29d45f9e2","permalink":"https://excv-workshop.github.io/publication/global-counterfactual-directions/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/global-counterfactual-directions/","section":"publication","summary":"Despite increasing progress in development of methods for generating visual counterfactual explanations, previous works consider them as an entirely local technique. In this work, we take the first step at globalizing them. Specifically, we discover that the latent space of Diffusion Autoencoders encodes the inference process of a given classifier in the form of global directions. We propose a novel proxy-based approach that discovers two types of these directions with the use of only single image in an entirely black-box manner. Precisely, g-directions allow for flipping the decision of a given classifier on an entire dataset of images, while h-directions further increase the diversity of explanations. We refer to them in general as Global Counterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally combined with Latent Integrated Gradients resulting in a new black-box attribution method, while simultaneously enhancing the understanding of counterfactual explanations. We validate our approach on existing benchmarks and show that it generalizes to real-world use-cases.","tags":["nonproceedings"],"title":"Global Counterfactual Directions","type":"publication"},{"authors":["Alina Elena Baia","Andrea Cavallaro"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"1715099fa9663b27224e5395f151465f","permalink":"https://excv-workshop.github.io/publication/image-guided-topic-modeling-for-interpretable-privacy-classification/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/image-guided-topic-modeling-for-interpretable-privacy-classification/","section":"publication","summary":"Predicting and explaining the private information contained in an image in human-understandable terms is a complex and contextual task. This task is challenging even for large language models. To facilitate the understanding of privacy decisions, we propose to predict image privacy based on a set of natural language content descriptors. These content descriptors are associated with privacy scores that reflect how people perceive image content. We generate descriptors with our novel Image-guided Topic Modeling (ITM) approach. ITM leverages, via multimodality alignment, both vision information and image textual descriptions from a vision language model. We use the ITM-generated descriptors to learn a privacy predictor, Priv$\times$ITM, whose decisions are interpretable by design. Our Priv$\times$ITM classifier outperforms the reference interpretable method by 5 percentage points in accuracy and performs comparably to the current non-interpretable state-of-the-art model.","tags":["proceedings"],"title":"Image-guided topic modeling for interpretable privacy classification","type":"publication"},{"authors":["David Debot","Giuseppe Marra"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"bb917831cf0ab5cdbffca97f4ac4a5c3","permalink":"https://excv-workshop.github.io/publication/integrating-local-and-global-interpretability-for-deep-concept-based-reasoning-models/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/integrating-local-and-global-interpretability-for-deep-concept-based-reasoning-models/","section":"publication","summary":"Two different approaches for interpretable Concept-Based Models (CBMs) exist: locally interpretable CBMs, which allow humans to understand the prediction of individual instances, and globally interpretable CBMs, which provide a broader understanding of their reasoning. In practice, the former focus on achieving high predictive accuracy, while the latter emphasize robustness and verifiability. To bridge this gap between extremes, we propose a hybrid model that integrates the strengths of both approaches. Our model, called Unified Concept Reasoner (UCR), leverages the high explainability of globally interpretable CBMs and high accuracy of locally interpretable CBMs, resulting in a powerful CBM with two heads that can be used for prediction. In our preliminary experimental evaluation, we show that UCR reaches comparable accuracy with competitors, converges to coherent global and local heads and is more stable w.r.t. hyperparameters.","tags":["proceedings"],"title":"Integrating Local and Global Interpretability for Deep Concept-Based Reasoning Models","type":"publication"},{"authors":["Mikołaj Sacha","Bartosz Jura","Dawid Damian Rymarczyk","Łukasz Struski","Jacek Tabor","Bartosz Michał Zieliński"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"2eb7a739e1c9b307b49ad5fb9de9d256","permalink":"https://excv-workshop.github.io/publication/interpretability-benchmark-for-evaluating-spatial-misalignment-of-prototypical-parts-explanations/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/interpretability-benchmark-for-evaluating-spatial-misalignment-of-prototypical-parts-explanations/","section":"publication","summary":"Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.","tags":["nonproceedings"],"title":"Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations","type":"publication"},{"authors":["Sagi Ben Itzhak","Nahum Kiryati","Orith Portnoy","Arnaldo Mayer"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"94859d947199af3014f3a4c34c0f83bf","permalink":"https://excv-workshop.github.io/publication/localization-guided-supervision-for-robust-medical-image-classification-by-vision-transformers/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/localization-guided-supervision-for-robust-medical-image-classification-by-vision-transformers/","section":"publication","summary":"A major challenge in developing data-driven algorithms for medical imaging is the limited size of available datasets. Furthermore, these datasets often suffer from inter-site heterogeneity caused by the use of different scanners and scanning protocols. These factors may contribute to overfitting, which undermines the generalization ability and robustness of deep learning classification models in the medical domain, leading to inadequate performance in real-world applications. To address these challenges and mitigate overfitting, we propose a framework which incorporates explanation supervision during training of Vision Transformer (ViT) models for image classification. Our approach leverages foreground masks of the class object during training to regularize attribution maps extracted from ViT, encouraging the model to focus on relevant image regions and make predictions based on pertinent features. We introduce a new method for generating explanatory attribution maps from ViT-based models and construct a dual-loss function that combines a conventional classification loss with a term that regularizes attribution maps. Our approach demonstrates superior performance over existing methods on two challenging medical imaging datasets, highlighting its effectiveness in the medical domain and its potential for application in other fields.","tags":["proceedings"],"title":"Localization-Guided Supervision for Robust Medical Image Classification by Vision Transformers","type":"publication"},{"authors":["Pirzada Suhail"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"2f3f8b0a2c895db3f25c958350d1c151","permalink":"https://excv-workshop.github.io/publication/network-inversion-of-binarised-neural-nets/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/network-inversion-of-binarised-neural-nets/","section":"publication","summary":"While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model’s learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous ”garbage” to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network’s structure, allowing for both inference and inversion.","tags":["nonproceedings"],"title":"Network Inversion of Binarised Neural Nets","type":"publication"},{"authors":["Ananthu Aniraj","Cassio Fraga Dantas","Dino Ienco","Diego Marcos"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"0bd6a4177749359d9d45602b4bd0c574","permalink":"https://excv-workshop.github.io/publication/pdiscoformer-relaxing-part-discovery-constraints-with-vision-transformers/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/pdiscoformer-relaxing-part-discovery-constraints-with-vision-transformers/","section":"publication","summary":"Computer vision methods that explicitly detect object parts and reason on them are a step towards inherently interpretable models. Existing approaches that perform part discovery driven by a fine-grained classification task make very restrictive assumptions on the geometric properties of the discovered parts; they should be small and compact. Although this prior is useful in some cases, in this paper we show that pre-trained transformer-based vision models, such as self-supervised DINOv2 ViT, enable the relaxation of these constraints. In particular, we find that a total variation (TV) prior, which allows for multiple connected components of any size, substantially outperforms previous work. We test our approach on three fine-grained classification benchmarks: CUB, PartImageNet and Oxford Flowers, and compare our results to previously published methods as well as a re-implementation of the state-of-the-art method PDiscoNet with a transformer-based backbone. We consistently obtain substantial improvements across the board, both on part discovery metrics and the downstream classification task, showing that the strong inductive biases in self-supervised ViT models require to rethink the geometric priors that can be used for unsupervised part discovery.","tags":["nonproceedings"],"title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers","type":"publication"},{"authors":["Vidya Prasad","Ruud Van Sloun","Anna Bartrolí","Nicola Pezzotti"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"82cee4e4bfcb2d33b675122c93ab19af","permalink":"https://excv-workshop.github.io/publication/proactiv-studying-deep-learning-model-behavior-under-input-transformations/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/proactiv-studying-deep-learning-model-behavior-under-input-transformations/","section":"publication","summary":"Deep learning (DL) models have shown performance benefits across many applications, from classification to image-to-image translation. However, low interpretability often leads to unexpected model behavior once deployed in the real world. Usually, this unexpected behavior is because the training data domain does not reflect the deployment data domain. Identifying a model's breaking points under input conditions and domain shifts, i.e., input transformations is essential to improve models. Although visual analytics (VA) has shown promise in studying the behavior of model outputs under continually varying inputs, existing methods mainly focus on per-class or instance-level analysis. We aim to generalize beyond classification where classes do not exist and provide a global view of model behavior under co-occurring input transformations. We present a DL model-agnostic VA method (ProactiV) to help model developers proactively study output behavior under input transformations to identify and verify breaking points. ProactiV relies on a proposed input optimization method to determine the changes to a given transformed input to achieve the desired output. The data from this optimization process allows the study of global and local model behavior under input transformations at scale. Additionally, the optimization method provides insights into the input characteristics that result in desired outputs and helps recognize model biases. We highlight how ProactiV effectively supports studying model behavior with example classification and image-to-image translation tasks.","tags":["nonproceedings"],"title":"ProactiV: Studying Deep Learning Model Behavior under Input Transformations","type":"publication"},{"authors":["Sayed Mohammad Vakilzadeh Hatefi","Maximilian Dreyer","Reduan Achtibat","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"4c99c31368f53918d0bf1a064698563b","permalink":"https://excv-workshop.github.io/publication/pruning-by-explaining-revisited-optimizing-attribution-methods-to-prune-cnns-and-transformers/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/pruning-by-explaining-revisited-optimizing-attribution-methods-to-prune-cnns-and-transformers/","section":"publication","summary":"To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of eXplainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.","tags":["proceedings"],"title":"Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers","type":"publication"},{"authors":["Thomas Norrenbrock","Marco Rudolph","Bodo Rosenhahn"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"a044866f2c4408bab5eaee7bcafcec62","permalink":"https://excv-workshop.github.io/publication/q-senn-quantized-self-explaining-neural-networks/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/q-senn-quantized-self-explaining-neural-networks/","section":"publication","summary":"Explanations in Computer Vision are often desired, but most Deep Neural Networks can only provide saliency maps with questionable faithfulness. Self-Explaining Neural Networks (SENN) extract interpretable concepts with fidelity, diversity, and grounding to combine them linearly for decision-making. While they can explain what was recognized, initial realizations lack accuracy and general applicability. We propose the Quantized-Self-Explaining Neural Network “Q-SENN”. Q-SENN satisfies or exceeds the desiderata of SENN while being applicable to more complex datasets and maintaining most or all of the accuracy of an uninterpretable baseline model, outperforming previous work in all considered metrics. Q-SENN describes the relationship between every class and feature as either positive, negative or neutral instead of an arbitrary number of possible relations, enforcing more binary human-friendly features. Since every class is assigned just 5 interpretable features on average, Q-SENN shows convincing local and global interpretability. Additionally, we propose a feature alignment method, capable of aligning learned features with human language-based concepts without additional supervision. Thus, what is learned can be more easily verbalized. The code is published: https://github.com/ThomasNorr/Q-SENN","tags":["nonproceedings"],"title":"Q-SENN: Quantized Self-Explaining Neural Networks","type":"publication"},{"authors":["Lars Nieradzik","Henrike Stephani","Janis Keuper"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"823eb15310f82f0736a7e346e192f9cb","permalink":"https://excv-workshop.github.io/publication/top-gap-integrating-size-priors-in-cnns-for-more-interpretability-robustness-and-bias-mitigation/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/top-gap-integrating-size-priors-in-cnns-for-more-interpretability-robustness-and-bias-mitigation/","section":"publication","summary":"This paper introduces Top-GAP, a novel regularization technique that enhances the explainability and robustness of convolutional neural networks. By constraining the spatial size of the learned feature representation, our method forces the network to focus on the most salient image regions, effectively reducing background influence. Using adversarial attacks and the Effective Receptive Field, we show that Top-GAP directs more attention towards object pixels rather than the background. This leads to enhanced interpretability and robustness. We achieve over 50% robust accuracy on CIFAR-10 with PGD $\u001bpsilon=\u000crac{8}{255}$ and $20$ iterations while maintaining the original clean accuracy. Furthermore, we see increases of up to 5% accuracy against distribution shifts. Our approach also yields more precise object localization, as evidenced by up to 25% improvement in Intersection over Union (IOU) compared to methods like GradCAM and Recipro-CAM.","tags":["proceedings"],"title":"Top-GAP: Integrating Size Priors in CNNs for more Interpretability, Robustness, and Bias Mitigation","type":"publication"},{"authors":["Matthew Kowal","Richard Wildes","Konstantinos G. Derpanis"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"503cd37c17bc312fb402fe213e6b4624","permalink":"https://excv-workshop.github.io/publication/visual-concept-connectome-vcc-open-world-concept-discovery-and-their-interlayer-connections-in-deep-models/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/visual-concept-connectome-vcc-open-world-concept-discovery-and-their-interlayer-connections-in-deep-models/","section":"publication","summary":"Understanding what deep network models capture in their learned representations is a fundamental challenge in computer vision. We present a new methodology to understanding such vision models, the Visual Concept Connectome (VCC), which discovers human interpretable concepts and their interlayer connections in a fully unsupervised manner. Our approach simultaneously reveals fine-grained concepts at a layer, connection weightings across all layers and is amendable to global analysis of network structure (e.g., branching pattern of hierarchical concept assemblies). Previous work yielded ways to extract interpretable concepts from single layers and examine their impact on classification, but did not afford multilayer concept analysis across an entire network architecture. Quantitative and qualitative empirical results show the effectiveness of VCCs in the domain of image classification. Also, we leverage VCCs for the application of failure mode debugging to reveal where mistakes arise in deep networks.","tags":["nonproceedings"],"title":"Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models","type":"publication"},{"authors":["Matthew Kowal","Achal Dave","Rares Andrei Ambrus","Adrien Gaidon","Konstantinos G. Derpanis","Pavel Tokmakov"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"05100071afefd2f9a0e3415706d30587","permalink":"https://excv-workshop.github.io/publication/vtcd-understanding-video-transformers-via-universal-concept-discovery/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/vtcd-understanding-video-transformers-via-universal-concept-discovery/","section":"publication","summary":"This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models. Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers. Finally, we demonstrate that VTCD can be used to improve model performance for fine-grained tasks.","tags":["nonproceedings"],"title":"VTCD: Understanding Video Transformers via Universal Concept Discovery","type":"publication"},{"authors":["Gabriela Csurka","Tyler L. Hayes","Diane Larlus","Riccardo Volpi"],"categories":null,"content":"","date":1725998523,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726869241,"objectID":"7c35787111d88796cc5fc41bce2863f5","permalink":"https://excv-workshop.github.io/publication/what-could-go-wrong-discovering-and-describing-failure-modes-in-computer-vision/","publishdate":"2024-09-10T22:02:03+02:00","relpermalink":"/publication/what-could-go-wrong-discovering-and-describing-failure-modes-in-computer-vision/","section":"publication","summary":"In this work, we propose a simple yet effective solution to predict and describe via natural language potential failure modes of computer vision models. Given a pretrained model and a set of samples, our aim is to find sentences that accurately describe the visual conditions in which the model under-performs. In order to study this important topic and foster future research on it, we formalize the problem of Language-Based Error Explainability (LBEE) and propose a set of metrics to evaluate and compare different methods for this task. We propose solutions that operate in a joint vision-and-language embedding space, and can characterize through language descriptions model failures caused, e.g., by objects unseen during training or adverse visual conditions.","tags":["proceedings"],"title":"What could go wrong? Discovering and describing failure modes in computer vision","type":"publication"}]